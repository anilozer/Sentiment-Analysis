  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Choice\n",
    "I analyzed and did a sentiment analysis task for Amazon Kindle Book Reviews. The dataset can be found at https://www.kaggle.com/datasets/meetnagadia/amazon-kindle-book-review-for-sentiment-analysis?resource=download\n",
    "\n",
    "I specifically chose and worked with the unpreprocessed version of the file and preprocessed it myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preprocess the data, I selected the columns that are relevant. Then I seperated the data for training, validation and test. Finally I tokenized the list that I derived from the columns and fed them to my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11539</td>\n",
       "      <td>B0033UV8HI</td>\n",
       "      <td>[8, 10]</td>\n",
       "      <td>3</td>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "      <td>09 2, 2010</td>\n",
       "      <td>A3HHXRELK8BHQG</td>\n",
       "      <td>Ridley</td>\n",
       "      <td>Entertaining But Average</td>\n",
       "      <td>1283385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5957</td>\n",
       "      <td>B002HJV4DE</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "      <td>10 8, 2013</td>\n",
       "      <td>A2RGNZ0TRF578I</td>\n",
       "      <td>Holly Butler</td>\n",
       "      <td>Terrific menage scenes!</td>\n",
       "      <td>1381190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9146</td>\n",
       "      <td>B002ZG96I4</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "      <td>04 11, 2014</td>\n",
       "      <td>A3S0H2HV6U1I7F</td>\n",
       "      <td>Merissa</td>\n",
       "      <td>Snapdragon Alley</td>\n",
       "      <td>1397174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7038</td>\n",
       "      <td>B002QHWOEU</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "      <td>07 5, 2014</td>\n",
       "      <td>AC4OQW3GZ919J</td>\n",
       "      <td>Cleargrace</td>\n",
       "      <td>very light murder cozy</td>\n",
       "      <td>1404518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1776</td>\n",
       "      <td>B001A06VJ8</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "      <td>12 31, 2012</td>\n",
       "      <td>A3C9V987IQHOQD</td>\n",
       "      <td>Rjostler</td>\n",
       "      <td>Book</td>\n",
       "      <td>1356912000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1        asin  helpful  rating  \\\n",
       "0           0         11539  B0033UV8HI  [8, 10]       3   \n",
       "1           1          5957  B002HJV4DE   [1, 1]       5   \n",
       "2           2          9146  B002ZG96I4   [0, 0]       3   \n",
       "3           3          7038  B002QHWOEU   [1, 3]       3   \n",
       "4           4          1776  B001A06VJ8   [0, 1]       4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Jace Rankin may be short, but he's nothing to ...   09 2, 2010   \n",
       "1  Great short read.  I didn't want to put it dow...   10 8, 2013   \n",
       "2  I'll start by saying this is the first of four...  04 11, 2014   \n",
       "3  Aggie is Angela Lansbury who carries pocketboo...   07 5, 2014   \n",
       "4  I did not expect this type of book to be in li...  12 31, 2012   \n",
       "\n",
       "       reviewerID  reviewerName                   summary  unixReviewTime  \n",
       "0  A3HHXRELK8BHQG        Ridley  Entertaining But Average      1283385600  \n",
       "1  A2RGNZ0TRF578I  Holly Butler   Terrific menage scenes!      1381190400  \n",
       "2  A3S0H2HV6U1I7F       Merissa          Snapdragon Alley      1397174400  \n",
       "3   AC4OQW3GZ919J    Cleargrace    very light murder cozy      1404518400  \n",
       "4  A3C9V987IQHOQD      Rjostler                      Book      1356912000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_kindle_review.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 11 columns):\n",
      "Unnamed: 0        12000 non-null int64\n",
      "Unnamed: 0.1      12000 non-null int64\n",
      "asin              12000 non-null object\n",
      "helpful           12000 non-null object\n",
      "rating            12000 non-null int64\n",
      "reviewText        12000 non-null object\n",
      "reviewTime        12000 non-null object\n",
      "reviewerID        12000 non-null object\n",
      "reviewerName      11962 non-null object\n",
      "summary           12000 non-null object\n",
      "unixReviewTime    12000 non-null int64\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the columns, we can see that the columns \"rating\" and \"reviewText\" are the ones that are relevant for our sentiment analysis. So we select these 2 columns as we proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         reviewText\n",
       "0       3  Jace Rankin may be short, but he's nothing to ...\n",
       "1       5  Great short read.  I didn't want to put it dow...\n",
       "2       3  I'll start by saying this is the first of four...\n",
       "3       3  Aggie is Angela Lansbury who carries pocketboo...\n",
       "4       4  I did not expect this type of book to be in li..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df[[\"rating\",\"reviewText\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks better. Now let us see if we have any null values that we have to get rid of, in order to have a clean data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating        0\n",
       "reviewText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good. Nothing to worry about.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3000\n",
       "4    3000\n",
       "3    2000\n",
       "2    2000\n",
       "1    2000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is not bad. Perhaps, for our sentiment analysis, we can say that the review scores 1, 2 and 3 are negative. 4 and 5 are positive. We would then have 6000 datapoints for each, which would make our data balanced :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"rating\"] = dataset[\"rating\"].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         reviewText\n",
       "0       0  Jace Rankin may be short, but he's nothing to ...\n",
       "1       1  Great short read.  I didn't want to put it dow...\n",
       "2       0  I'll start by saying this is the first of four...\n",
       "3       0  Aggie is Angela Lansbury who carries pocketboo...\n",
       "4       1  I did not expect this type of book to be in li..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6000\n",
       "0    6000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last steps of preprocessing\n",
    "dataset['reviewText'] = dataset['reviewText'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = dataset[\"reviewText\"].tolist()\n",
    "rating = dataset[\"rating\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 7200 # 60 % of the data for training\n",
    "validation_samples = 2400 # 20 % of the data for validation\n",
    "test_samples = 2400 # 20 % of the data for test\n",
    "maxwords = 20000 # I should only consider the 20 000 most frequent words.\n",
    "maxlen = 500 # appropriate maximum length of sequences for a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shows 33056 different tokens.\n"
     ]
    }
   ],
   "source": [
    "# The most important step of data processing in sentiment analysis : tokenization\n",
    "tokenizer = Tokenizer(num_words = maxwords) #words for tokenization are the 20 000 most frequent words\n",
    "tokenizer.fit_on_texts(review) # indexing every word\n",
    "sequences = tokenizer.texts_to_sequences(review) # encoding each word to its corresponding number\n",
    "\n",
    "word_index = tokenizer.word_index     # word index for specificity\n",
    "print('Model shows %s different tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the review tensor is: (12000, 500)\n",
      "The shape of the rating tensor is: (12000,)\n"
     ]
    }
   ],
   "source": [
    "review = pad_sequences(sequences, maxlen = maxlen) # all the reviews are now max 100 words\n",
    "rating = np.asarray(rating) #  review ratings as vectors\n",
    "print('The shape of the review tensor is:', review.shape)\n",
    "print('The shape of the rating tensor is:', rating.shape)\n",
    "rating = to_categorical(rating) # converting the class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shuffle to assure randomization\n",
    "indexes = np.arange(review.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "review = review[indexes]\n",
    "rating = rating[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for training, validation and testing\n",
    "\n",
    "#training\n",
    "review_train = review[:training_samples]\n",
    "rating_train = rating[:training_samples]\n",
    "\n",
    "#validation\n",
    "review_val = review[training_samples: training_samples + validation_samples]\n",
    "rating_val = rating[training_samples: training_samples + validation_samples]\n",
    "\n",
    "#testing\n",
    "review_test = review[validation_samples + test_samples::]\n",
    "rating_test = rating[validation_samples + test_samples::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pretrained word vector, I am using the Wikipedia 2014 and Gigaword 5 model that I acquired from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"glove.6B.100d.txt\", \"r\",encoding = 'utf-8' )\n",
    "dic = {}\n",
    "\n",
    "for line in file: #going through every line in the file\n",
    "    values = line.split(' ') #splitting all the values\n",
    "    word = values[0] #the word as the first value\n",
    "    idd = np.asarray(values[1:], dtype='float32') #creating a numpy array with words and their indexes\n",
    "    dic[word] = idd #words as keys and idds as values into empty dictionary dic\n",
    "    \n",
    "file.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This glove has 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('This glove has %s word vectors.' % len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((maxwords, embedding_dim)) # a new array filled with zeros\n",
    "for word, i in word_index.items():   \n",
    "    embedding_vector = dic.get(word)\n",
    "    if i < maxwords:\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup,Training and Test of our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading several articles and listening to some experts, my research led to the fact that LSTM type of language model is the best approach towards doing sentiment analysis. LSTM is a specific kind of RNN that can understand the long term dependence between words and this is crucial in sentiment analysis where I have to establish relationships between words in a specific corpus via word embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setting up, training and testing of our model, I will play with the parameters of my model to observe which parameters have an effect on the accuracy and loss values of my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,042,370\n",
      "Trainable params: 2,042,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential() # setting up a sequential model\n",
    "model.add(Embedding(maxwords, embedding_dim, input_length=maxlen)) # use of embedding layer\n",
    "model.add(LSTM(64)) #one hidden LSTM layer with 64 hidden units\n",
    "model.add(Dense(2, activation='softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False # Let us make sure that the embedding layer does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 39s 331ms/step - loss: 0.6271 - acc: 0.6454 - val_loss: 0.5595 - val_acc: 0.7142\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 37s 326ms/step - loss: 0.5612 - acc: 0.7199 - val_loss: 0.5302 - val_acc: 0.7462\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 36s 319ms/step - loss: 0.5267 - acc: 0.7492 - val_loss: 0.4926 - val_acc: 0.7633\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 36s 319ms/step - loss: 0.5109 - acc: 0.7565 - val_loss: 0.5227 - val_acc: 0.7317\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 36s 321ms/step - loss: 0.4867 - acc: 0.7710 - val_loss: 0.5161 - val_acc: 0.7617\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 36s 322ms/step - loss: 0.4657 - acc: 0.7885 - val_loss: 0.5875 - val_acc: 0.7158\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 36s 318ms/step - loss: 0.4457 - acc: 0.7951 - val_loss: 0.6477 - val_acc: 0.6946\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 35s 313ms/step - loss: 0.4322 - acc: 0.8042 - val_loss: 0.4425 - val_acc: 0.7983\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 36s 315ms/step - loss: 0.4166 - acc: 0.8119 - val_loss: 0.4724 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 35s 314ms/step - loss: 0.4048 - acc: 0.8200 - val_loss: 0.5210 - val_acc: 0.7721\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(review_train, rating_train,\n",
    "                    epochs = 10, #over 10 epochs\n",
    "                    batch_size = 64,\n",
    "                    validation_data = (review_val, rating_val)) # specifying the validation data \n",
    "\n",
    "#save the weights\n",
    "model.save_weights('pre_trained_wiki_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 12s 54ms/step - loss: 0.4927 - acc: 0.7824\n",
      "test_acc: 0.7823610901832581\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(review_test, rating_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 1\n",
    "In my first model with one lstm of 64 hidden units, standard 10 epochs and the batch size of 64, my model had .78 test accuracy, which means that it evaluates 78 out of every 100 sentiments correctly (evaluating it as positive when it is positive or vice versa). It is not bad. However, the loss value is simply too high. This means that we get a lot of errors and our model does not do a great job.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 2,003,506\n",
      "Trainable params: 2,003,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(maxwords, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(8)) #now, just with 8 units\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "113/113 [==============================] - 20s 163ms/step - loss: 0.6588 - acc: 0.6089 - val_loss: 0.6210 - val_acc: 0.6646\n",
      "Epoch 2/5\n",
      "113/113 [==============================] - 18s 159ms/step - loss: 0.5956 - acc: 0.6892 - val_loss: 0.5792 - val_acc: 0.7017\n",
      "Epoch 3/5\n",
      "113/113 [==============================] - 18s 156ms/step - loss: 0.5528 - acc: 0.7276 - val_loss: 0.5405 - val_acc: 0.7346\n",
      "Epoch 4/5\n",
      "113/113 [==============================] - 18s 160ms/step - loss: 0.5219 - acc: 0.7464 - val_loss: 0.5115 - val_acc: 0.7533\n",
      "Epoch 5/5\n",
      "113/113 [==============================] - 18s 155ms/step - loss: 0.4992 - acc: 0.7657 - val_loss: 0.5131 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(review_train, rating_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(review_val, rating_val))\n",
    "model.save_weights('pre_trained_wiki_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 32ms/step - loss: 0.5071 - acc: 0.7583\n",
      "test_acc: 0.7583333253860474\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(review_test, rating_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 2\n",
    "After reducing the lstm units down to 8 and epochs down to 5, the accuracy is even lower and the loss value is even higher. This means that this model is definitely worse than the previous one. This is not the approach we would like to take to improve our model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,117,506\n",
      "Trainable params: 2,117,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(maxwords, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(128)) # with an increased amount of 128 units\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 46s 784ms/step - loss: 0.6495 - acc: 0.6322 - val_loss: 0.6011 - val_acc: 0.6925\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 44s 777ms/step - loss: 0.5829 - acc: 0.7018 - val_loss: 0.5432 - val_acc: 0.7404\n",
      "Epoch 3/15\n",
      "57/57 [==============================] - 44s 780ms/step - loss: 0.5499 - acc: 0.7279 - val_loss: 0.6882 - val_acc: 0.6217\n",
      "Epoch 4/15\n",
      "57/57 [==============================] - 44s 773ms/step - loss: 0.5319 - acc: 0.7419 - val_loss: 0.4981 - val_acc: 0.7608\n",
      "Epoch 5/15\n",
      "57/57 [==============================] - 44s 768ms/step - loss: 0.5134 - acc: 0.7540 - val_loss: 0.7248 - val_acc: 0.6475\n",
      "Epoch 6/15\n",
      "57/57 [==============================] - 44s 771ms/step - loss: 0.4958 - acc: 0.7701 - val_loss: 0.4771 - val_acc: 0.7721\n",
      "Epoch 7/15\n",
      "57/57 [==============================] - 44s 773ms/step - loss: 0.4789 - acc: 0.7763 - val_loss: 0.4768 - val_acc: 0.7767\n",
      "Epoch 8/15\n",
      "57/57 [==============================] - 44s 778ms/step - loss: 0.4659 - acc: 0.7835 - val_loss: 0.4649 - val_acc: 0.7833\n",
      "Epoch 9/15\n",
      "57/57 [==============================] - 44s 775ms/step - loss: 0.4470 - acc: 0.7953 - val_loss: 0.4491 - val_acc: 0.7937\n",
      "Epoch 10/15\n",
      "57/57 [==============================] - 44s 773ms/step - loss: 0.4269 - acc: 0.8085 - val_loss: 0.4668 - val_acc: 0.7792\n",
      "Epoch 11/15\n",
      "57/57 [==============================] - 44s 769ms/step - loss: 0.4134 - acc: 0.8172 - val_loss: 0.4605 - val_acc: 0.7896\n",
      "Epoch 12/15\n",
      "57/57 [==============================] - 44s 777ms/step - loss: 0.3885 - acc: 0.8288 - val_loss: 0.4542 - val_acc: 0.7933\n",
      "Epoch 13/15\n",
      "57/57 [==============================] - 44s 768ms/step - loss: 0.3729 - acc: 0.8333 - val_loss: 0.4679 - val_acc: 0.7887\n",
      "Epoch 14/15\n",
      "57/57 [==============================] - 43s 763ms/step - loss: 0.3536 - acc: 0.8422 - val_loss: 0.6447 - val_acc: 0.7071\n",
      "Epoch 15/15\n",
      "57/57 [==============================] - 44s 772ms/step - loss: 0.3333 - acc: 0.8546 - val_loss: 0.4848 - val_acc: 0.7767\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(review_train, rating_train,\n",
    "                    epochs=15,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(review_val, rating_val))\n",
    "model.save_weights('pre_trained_wiki_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 33s 143ms/step - loss: 0.4356 - acc: 0.8032\n",
      "test_acc: 0.8031944632530212\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(review_test, rating_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 3\n",
    "Wow, it took a while to run this model, thanks to the high batch size of 128 and 15 epochs. Also the LSTM now has 128 hidden units. \n",
    "This all seemed to help the model. Our test accuracy is over 80 percent, which means more than 8 out of 10 sentiment analyses are done correctly by our model. The loss value is smaller as well, so the countability of the model is higher than the previous 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,042,370\n",
      "Trainable params: 2,042,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(maxwords, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(64))  \n",
    "model.add(Dense(2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 37s 320ms/step - loss: 0.7233 - acc: 0.6793 - val_loss: 0.7753 - val_acc: 0.7763\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 36s 319ms/step - loss: 0.5488 - acc: 0.8297 - val_loss: 0.8427 - val_acc: 0.8258\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 36s 317ms/step - loss: 0.5426 - acc: 0.8836 - val_loss: 0.9182 - val_acc: 0.8096\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 36s 321ms/step - loss: 1.3671 - acc: 0.7546 - val_loss: 0.6427 - val_acc: 0.7275\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 36s 318ms/step - loss: 2.4394 - acc: 0.6140 - val_loss: 3.2681 - val_acc: 0.5496\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 36s 315ms/step - loss: 1.1223 - acc: 0.7599 - val_loss: 1.1047 - val_acc: 0.7858\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 36s 318ms/step - loss: 0.6213 - acc: 0.8542 - val_loss: 0.8041 - val_acc: 0.7858\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 36s 318ms/step - loss: 0.4187 - acc: 0.9143 - val_loss: 0.7010 - val_acc: 0.8296\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 36s 317ms/step - loss: 0.3353 - acc: 0.9292 - val_loss: 1.0259 - val_acc: 0.8183\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 36s 317ms/step - loss: 0.4823 - acc: 0.9463 - val_loss: 1.6810 - val_acc: 0.8017\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(review_train, rating_train,\n",
    "                    epochs = 10, # over 10 epochs\n",
    "                    batch_size = 64,\n",
    "                    validation_data = (review_val, rating_val)) # specifying the validation data \n",
    "\n",
    "#save the weights\n",
    "model.save_weights('pre_trained_wiki_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 12s 53ms/step - loss: 1.2825 - acc: 0.8532\n",
      "test_acc: 0.8531944155693054\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(review_test, rating_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result 4 \n",
    "On my final comparison, I wanted to compare the activation functions and see what kind of effect it has on the model. On this model, I kept all the values same as the first model, other than the activation function, which is now default. \n",
    "\n",
    "I have found out out that when we do not define the activation function in the model, it is a \"tanh\" function with the recurrent activation of sigmoid. So this trial does not have softmax as its activator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that through the epochs, the model significantly changes and is under serious pressure. In the end, we reach almost 95% accuracy but the validation loss value is huge. \n",
    "In total, the model has a test accuracy of 85%, which is the highest value we have achieved so far. But this value should not deceive us. The loss value is over 1, which means that we can not trust this type of model to make decisions on sentiment analysis. Therefore, I conclude that softmax function is better than the default tanh function for our task of sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks:\n",
    "\n",
    "I would have desired to reach higher accuracy/test accuracy and lower loss values. I would not be super confident on this model. One interesting task would be to change the Wikipedia glove that I used to a more recent and inclusive glove and see whether the model does a better job. Wikipedia glove had 400 thousand entries vs some models having millions. A more updated glove can result in higher accuracy and lower loss values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyKeras)",
   "language": "python",
   "name": "kerasenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
